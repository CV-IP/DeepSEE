{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepSEE: Deep Disentangled Semantic Explorative Extreme Super-Resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a demo for our paper **Deep Disentangled Semantic Explorative Extreme Super-Resolution** ([ACCV 2020](http://accv2020.kyoto/) oral). \n",
    "Please check out our [project page](https://mcbuehler.github.io/DeepSEE/) for details and the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "1. Download and unpack the model checkpoints.\n",
    "2. Download and unpack the demo data.\n",
    "3. Install the requirements.\n",
    "\n",
    "Please refer to the [README](README.md) for instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from demo import Demo, get_demo_options, display_result\n",
    "import torch\n",
    "import numpy as np\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "opt = get_demo_options(\"32x_independent\")\n",
    "demo = Demo(opt)\n",
    "base_path = \"demo_data/\"\n",
    "dataset = \"CelebAMask-HQ\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference for the __Default__ Solution\n",
    "We upscale a low-resolution (16x16) image to a single high-resolution variant for 32x upscaling. We call this the __default solution__.\n",
    "\n",
    "We provide these sample images from the [CelebAMask-HQ](https://github.com/switchablenorms/CelebAMask-HQ) dataset (bicubically downscaled to 16x16 pixels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['14337.png', '19776.png', '28368.png']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.path.join(base_path, dataset, \"image_16x16\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the default solution, we need to provide two inputs:\n",
    "* the low-resolution input and \n",
    "* its predicted semantic mask (pre-computed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding style from LR image...\n",
      "Style computed.\n",
      "Upscaling...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buehlmar/data/software/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:1614: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89282ea7ab9c45a1b775d75ebe6a4914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Visualize:', index=1, options=('encoded_style', 'fake_image', 'ima…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename = \"14337.png\"\n",
    "\n",
    "kwargs = {\n",
    "    \"name\": \"demo\",\n",
    "    \"path_image_lr\": os.path.join(base_path, dataset, \"image_16x16\", filename),\n",
    "    \"path_semantics\": os.path.join(base_path, dataset, \"predicted_labels\", filename)\n",
    "}\n",
    "result_default_solution = demo.run(**kwargs)\n",
    "display_result(result_default_solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Manipulations\n",
    "We can repaint the semantic mask and run inference again. This yields different shapes in the upscaled image, but preserves the overall appearance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding style from LR image...\n",
      "Style computed.\n",
      "Upscaling...\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107036ec4c89498db8efa106b2298cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Visualize:', index=1, options=('encoded_style', 'fake_image', 'ima…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kwargs = {\n",
    "    \"name\": \"demo_semantic_manipulation\",\n",
    "    \"path_image_lr\": os.path.join(base_path, dataset, \"image_16x16\", filename),\n",
    "    \"path_semantics\": os.path.join(base_path, dataset, \"manipulated_labels\", filename)\n",
    "}\n",
    "result_default_solution = demo.run(**kwargs)\n",
    "display_result(result_default_solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style Manipulations\n",
    "We can add random noise to generate multiple high-resolution variants. In the following example, we change the smooth skin texture by adding noise to the corresponding row in the style matrix.\n",
    "\n",
    "* Change `delta` to make the effect stronger or weaker.\n",
    "* You can choose to influence other/multiple semantic regions by changing/adding indices to `region`.\n",
    "* The style matrix (or rows thereof) can be computed from other high-resolution images (examples in the paper).\n",
    "* Rows in the style matrix can be interpolated (examples in the paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upscaling...\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d91d4f5671b34a11a42ffb383cce4193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Visualize:', index=1, options=('encoded_style', 'fake_image', 'ima…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_style_orig = result_default_solution[\"encoded_style\"].detach().clone()\n",
    "encoded_style_noisy = encoded_style_orig.clone()\n",
    "import torch\n",
    "delta = 0.15\n",
    "region = [1]  # Corresponds to the skin region.\n",
    "\n",
    "encoded_style_noisy[:, region, :] = (encoded_style_orig[:, region, :] + (torch.rand(encoded_style_orig[:, region, :].shape) * delta)).clamp(-1, 1)\n",
    "kwargs = {\n",
    "    \"name\": \"demo_style_manipulation\",\n",
    "    \"path_image_lr\": os.path.join(base_path, dataset, \"image_16x16\", filename),\n",
    "    \"path_semantics\": os.path.join(base_path, dataset, \"manipulated_labels\", filename),\n",
    "    \"encoded_style\": encoded_style_noisy\n",
    "}\n",
    "result = demo.run(**kwargs)\n",
    "display_result(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
