{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepSEE: Deep Disentangled Semantic Explorative Extreme Super-Resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a demo for our paper **Deep Disentangled Semantic Explorative Extreme Super-Resolution** ([ACCV 2020](http://accv2020.kyoto/) oral). \n",
    "\n",
    "Please check out our [project page](https://mcbuehler.github.io/DeepSEE/) for details and the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "1. Download and unpack the model checkpoints to the folder `checkpoints`. <br>\n",
    "https://drive.google.com/drive/folders/1zZdvCaPExdM51Znw9-4Ku2PDpL0P1Klf?usp=sharing\n",
    "\n",
    "2. Download and unpack the demo data to the folder `demo_data`.<br>\n",
    "https://drive.google.com/drive/folders/1shKT0pDmIPZDpBvTgQhP-Q8juqD_Xsyu?usp=sharing\n",
    "\n",
    "3. Install the requirements.\n",
    "```\n",
    "conda install --file requirements.txt\n",
    "# Not all packages might be available in conda. Therefore, install them via pip:\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Please refer to the [README](README.md) for instructions and download links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from demo import Demo, get_demo_options, display_result\n",
    "import torch\n",
    "import numpy as np\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "opt = get_demo_options(\"32x_independent\")\n",
    "demo = Demo(opt)\n",
    "base_path = \"demo_data/\"\n",
    "dataset = \"CelebAMask-HQ\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method\n",
    "\n",
    "A low-resolution input ($x_{lr} \\in \\mathbb{R}^{H_{lr}\\times{W_{lr}}\\times{3}}$) image acts as a starting point that carries the low-frequency information. A generator ($G_{\\Theta}$) upscales this image and hallucinates the high-frequencies yielding the high-resolution image $\\hat x_{hr} \\in \\mathbb{R}^{H_{hr}\\times{W_{hr}}\\times{3}}$. As a guidance, $G_{\\Theta}$ leverages both a high resolution semantic map ($M \\in \\mathbb{R}^{H_{hr}\\times{W_{hr}}\\times{N}}$, where $N$ is the number of the semantic regions) and independent styles per region ($S\\in\\mathbb{R}^{N\\times d}$, where $d$ is the style dimensionality). The upscaled image should thus retain the low-frequency information from the low-resolution image. In addition, it should be consistent in terms of the semantic regions and have specific, yet independent styles per region. We formally define our problem as\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat x_{hr} = G_{\\Theta}(x_{lr}, \\thinspace M, \\thinspace S).\n",
    "    \\label{equation:formulation}\n",
    "\\end{equation}\n",
    "\n",
    "A user is able to control the \\textit{appearance} and \\textit{shape} of each semantic region. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Results\n",
    "\n",
    "### Inference for the __Default__ Solution\n",
    "We upscale a low-resolution (16x16) image to a high-resolution (512x512). We call this the __default solution__.\n",
    "\n",
    "We provide these sample images from the [CelebAMask-HQ](https://github.com/switchablenorms/CelebAMask-HQ) dataset (bicubically downscaled to 16x16 pixels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['14337.png', '19776.png', '28368.png']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.path.join(base_path, dataset, \"image_16x16\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the default solution, the model requires two inputs:\n",
    "1. the low-resolution image $x_{lr}$ and \n",
    "2. a semantic mask $M$ (we predicted the semantic mask from the low-resolution image)\n",
    "\n",
    "The style matrix $S$ is predicted from the low-resolution image $x_{lr}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding style from LR image...\n",
      "Style computed.\n",
      "Upscaling...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buehlmar/data/software/anaconda3/envs/tmp2/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9dd465fed574bbab78a563aa78d28b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Visualize:', index=1, options=('encoded_style', 'fake_image', 'ima…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename = \"14337.png\"\n",
    "\n",
    "kwargs = {\n",
    "    \"name\": \"demo\",\n",
    "    \"path_image_lr\": os.path.join(base_path, dataset, \"image_16x16\", filename),\n",
    "    \"path_semantics\": os.path.join(base_path, dataset, \"predicted_labels\", filename)\n",
    "}\n",
    "result_default_solution = demo.run(**kwargs)\n",
    "display_result(result_default_solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Manipulations\n",
    "We can repaint the semantic mask and run inference again. This yields different shapes in the upscaled image, but preserves the overall appearance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding style from LR image...\n",
      "Style computed.\n",
      "Upscaling...\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab982179f3f40d585ce6386a5212d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Visualize:', index=1, options=('encoded_style', 'fake_image', 'ima…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kwargs = {\n",
    "    \"name\": \"demo_semantic_manipulation\",\n",
    "    \"path_image_lr\": os.path.join(base_path, dataset, \"image_16x16\", filename),\n",
    "    \"path_semantics\": os.path.join(base_path, dataset, \"manipulated_labels\", filename)\n",
    "}\n",
    "result_default_solution = demo.run(**kwargs)\n",
    "display_result(result_default_solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style Manipulations\n",
    "We can add random noise to generate multiple high-resolution variants. In the following example, we change the smooth skin texture by adding noise to the corresponding row in the style matrix.\n",
    "\n",
    "* Change `delta` to make the effect stronger or weaker.\n",
    "* You can choose to influence other/multiple semantic regions by changing/adding indices to `region`.\n",
    "* The style matrix (or rows thereof) can be computed from other high-resolution images (examples in the paper).\n",
    "* Rows in the style matrix can be interpolated (examples in the paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upscaling...\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c4d0a6fc8624c1aab918e12daa247cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Visualize:', index=1, options=('encoded_style', 'fake_image', 'ima…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_style_orig = result_default_solution[\"encoded_style\"].detach().clone()\n",
    "encoded_style_noisy = encoded_style_orig.clone()\n",
    "import torch\n",
    "delta = 0.15\n",
    "region = [1]  # Corresponds to the skin region.\n",
    "\n",
    "encoded_style_noisy[:, region, :] = (encoded_style_orig[:, region, :] + (torch.rand(encoded_style_orig[:, region, :].shape) * delta)).clamp(-1, 1)\n",
    "kwargs = {\n",
    "    \"name\": \"demo_style_manipulation\",\n",
    "    \"path_image_lr\": os.path.join(base_path, dataset, \"image_16x16\", filename),\n",
    "    \"path_semantics\": os.path.join(base_path, dataset, \"manipulated_labels\", filename),\n",
    "    \"encoded_style\": encoded_style_noisy\n",
    "}\n",
    "result = demo.run(**kwargs)\n",
    "display_result(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
